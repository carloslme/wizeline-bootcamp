{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "user_behavior.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNB2gCkLGWaSfyVYL8jQkDh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carloslme/wizeline-bootcamp/blob/main/pyspark/user_behavior.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_OeoEozXrvu"
      },
      "source": [
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Parquet imports\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZokoqPCsXvMa"
      },
      "source": [
        "# Install dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz \n",
        "!tar -xvf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install fsspec\n",
        "!pip install gcsfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FEtwGbvXx76"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "\n",
        "sc = SparkContext()\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMuB0ovLbK-A"
      },
      "source": [
        "from google.oauth2 import service_account\n",
        "from google.cloud.storage import client\n",
        "import io\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "import json\n",
        "import filecmp"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sodDksfFbMXO"
      },
      "source": [
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    '/content/dataproc_service_account.json',\n",
        "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
        ")\n",
        "\n",
        "client = client.Client(\n",
        "    credentials=credentials,\n",
        "    project=credentials.project_id,\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdK4QH-Fbu0r"
      },
      "source": [
        "BUCKET = 'staging-layer-330021'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es9Ok5eXbotM"
      },
      "source": [
        "def download_file(local_filename, remote_filename):\n",
        "    bucket = client.get_bucket(BUCKET)\n",
        "    blob = bucket.blob(remote_filename)\n",
        "    blob.download_to_filename(local_filename)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsPPmqu7ZCUd"
      },
      "source": [
        "download_file(\"/content/user_purchase.csv\",\"user_purchase.csv\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH33PcO3XywR"
      },
      "source": [
        "df = spark.read.options(header=True).csv('/movie_review.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}