{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "user_behavior.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyME9Cw8oqXi6qN7IOdtaKeh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carloslme/wizeline-bootcamp/blob/main/pyspark/user_behavior.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_OeoEozXrvu"
      },
      "source": [
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Parquet imports\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZokoqPCsXvMa"
      },
      "source": [
        "# Install dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz \n",
        "!tar -xvf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install fsspec\n",
        "!pip install gcsfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FEtwGbvXx76"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "\n",
        "sc = SparkContext()\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMuB0ovLbK-A"
      },
      "source": [
        "from google.oauth2 import service_account\n",
        "from google.cloud.storage import client\n",
        "import io\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "import json\n",
        "import filecmp"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sodDksfFbMXO"
      },
      "source": [
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    '/content/gcs_service_account.json',\n",
        "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
        ")\n",
        "\n",
        "client = client.Client(\n",
        "    credentials=credentials,\n",
        "    project=credentials.project_id,\n",
        ")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdK4QH-Fbu0r"
      },
      "source": [
        "BUCKET = 'staging-layer-330021'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es9Ok5eXbotM"
      },
      "source": [
        "def download_file(local_filename, remote_filename):\n",
        "    bucket = client.get_bucket(BUCKET)\n",
        "    blob = bucket.blob(remote_filename)\n",
        "    blob.download_to_filename(local_filename)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsPPmqu7ZCUd"
      },
      "source": [
        "download_file(\"/content/user_purchase.csv\",\"user_purchase.csv\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsD6MsiCcojM"
      },
      "source": [
        "download_file(\"/content/part-00000-59bf5da5-e43a-4eba-8c9b-153c96ee2a85-c000.snappy.parquet\", \"reviews.parquet/part-00000-59bf5da5-e43a-4eba-8c9b-153c96ee2a85-c000.snappy.parquet\")\n",
        "download_file(\"/content/part-00001-59bf5da5-e43a-4eba-8c9b-153c96ee2a85-c000.snappy.parquet\", \"reviews.parquet/part-00001-59bf5da5-e43a-4eba-8c9b-153c96ee2a85-c000.snappy.parquet\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH33PcO3XywR"
      },
      "source": [
        "df_reviews = spark.read.options(header=True).parquet('*.parquet')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm-nSdIee771"
      },
      "source": [
        "df_user_purchase = spark.read.options(header=True).csv('*.csv')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOUfESiIe_KV",
        "outputId": "11dcb401-8323-44b4-ee9a-ac753235b064"
      },
      "source": [
        "df_reviews.columns"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['user_id', 'positive_review']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx0SEgVJicm2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562d88f5-77ee-4cae-82f2-c108ed001488"
      },
      "source": [
        "df_user_purchase.columns"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['invoice_number',\n",
              " 'stock_code',\n",
              " 'detail',\n",
              " 'quantity',\n",
              " 'invoice_date',\n",
              " 'unit_price',\n",
              " 'customer_id',\n",
              " 'country']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APHqfTPqhalN"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "data = (\n",
        "    df_reviews.join(df_user_purchase, df_reviews.user_id == df_user_purchase.customer_id).select(df_reviews[\"positive_review\"], df_user_purchase[\"*\"])\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdMXYO4whhvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2edf4ae7-2299-4aec-c549-3827acf61d93"
      },
      "source": [
        "data.show(10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------+----------+--------------------+--------+------------+----------+-----------+--------------+\n",
            "|positive_review|invoice_number|stock_code|              detail|quantity|invoice_date|unit_price|customer_id|       country|\n",
            "+---------------+--------------+----------+--------------------+--------+------------+----------+-----------+--------------+\n",
            "|              0|        536365|    85123A|WHITE HANGING HEA...|       6|1291191960.0|      2.55|      17850|United Kingdom|\n",
            "|              1|        536365|    85123A|WHITE HANGING HEA...|       6|1291191960.0|      2.55|      17850|United Kingdom|\n",
            "|              0|        536365|    85123A|WHITE HANGING HEA...|       6|1291191960.0|      2.55|      17850|United Kingdom|\n",
            "|              0|        536365|    85123A|WHITE HANGING HEA...|       6|1291191960.0|      2.55|      17850|United Kingdom|\n",
            "|              0|        536365|    85123A|WHITE HANGING HEA...|       6|1291191960.0|      2.55|      17850|United Kingdom|\n",
            "|              0|        536365|    85123A|WHITE HANGING HEA...|       6|1291191960.0|      2.55|      17850|United Kingdom|\n",
            "|              0|        536365|    85123A|WHITE HANGING HEA...|       6|1291191960.0|      2.55|      17850|United Kingdom|\n",
            "|              1|        536365|    85123A|WHITE HANGING HEA...|       6|1291191960.0|      2.55|      17850|United Kingdom|\n",
            "|              1|        536365|    85123A|WHITE HANGING HEA...|       6|1291191960.0|      2.55|      17850|United Kingdom|\n",
            "|              0|        536365|    85123A|WHITE HANGING HEA...|       6|1291191960.0|      2.55|      17850|United Kingdom|\n",
            "+---------------+--------------+----------+--------------------+--------+------------+----------+-----------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_jutf-_inpV"
      },
      "source": [
        "# customerid   => user_purchase.customerid,\n",
        "# amount_spent => SUM(user_purchase.quantity * user_purchase.unit_price),\n",
        "# review_score => SUM(reviews.positive_review),\n",
        "# review_count => COUNT(reviews.id),                            \n",
        "# insert_date  => airflow timestamp"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-H9Djhmmo0X"
      },
      "source": [
        "df_reviews.createOrReplaceTempView(\"reviews\")\n",
        "df_user_purchase.createOrReplaceTempView(\"user_purchase\")"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFkhsOFonKMI",
        "outputId": "3c66d422-d0cd-4fb2-f0b8-6602a3981585"
      },
      "source": [
        "spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT \n",
        "      up.customer_id AS customer\n",
        "      , SUM(up.quantity * up.unit_price) as amount_spent\n",
        "      , SUM(r.positive_review) as review_score\n",
        "      , COUNT(r.user_id) as review_count\n",
        "      , current_date() as insert_date\n",
        "    FROM reviews AS r\n",
        "    LEFT JOIN user_purchase AS up ON r.user_id == up.customer_id\n",
        "    GROUP BY 1\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        ").show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------------+------------+------------+-----------+\n",
            "|customer|      amount_spent|review_score|review_count|insert_date|\n",
            "+--------+------------------+------------+------------+-----------+\n",
            "|   15271|246096.18000000415|       12375|       27225| 2021-12-02|\n",
            "|   15555| 575742.2000000098|       50875|      111925| 2021-12-02|\n",
            "|   15574| 95505.99999999267|       10416|       22848| 2021-12-02|\n",
            "|   16250| 37775.67999999979|         744|        2328| 2021-12-02|\n",
            "|   17551| 36820.80000000055|        2021|        5160| 2021-12-02|\n",
            "|   17757|  575305.470000109|       31906|       76426| 2021-12-02|\n",
            "|   14525|426358.37000001606|       11026|       30098| 2021-12-02|\n",
            "|   13174| 307056.2700000226|       12874|       34226| 2021-12-02|\n",
            "|   14639|351328.46000000875|        7875|       20825| 2021-12-02|\n",
            "|   14810|206700.12000000363|        8160|       25245| 2021-12-02|\n",
            "+--------+------------------+------------+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w2yvSYqtniE"
      },
      "source": [
        "logic = spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT \n",
        "      CAST(up.customer_id AS INTEGER) AS customer\n",
        "      , SUM(up.quantity * up.unit_price) as amount_spent\n",
        "      , SUM(r.positive_review) as review_score\n",
        "      , COUNT(r.user_id) as review_count\n",
        "      , current_date() as insert_date\n",
        "    FROM reviews AS r\n",
        "    LEFT JOIN user_purchase AS up ON r.user_id == up.customer_id\n",
        "    GROUP BY 1\n",
        "    LIMIT 10\n",
        "    \"\"\"\n",
        ")"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaqZ6veJ6LN3",
        "outputId": "8581cec0-a8ab-4088-f78a-18610397c2da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "logic.dtypes"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('customer', 'int'),\n",
              " ('amount_spent', 'double'),\n",
              " ('review_score', 'bigint'),\n",
              " ('review_count', 'bigint'),\n",
              " ('insert_date', 'date')]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VVQOmZoo8gM"
      },
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
        "\n",
        "pd_df = logic.toPandas()"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-4Ecs-juhh3"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Construct a BigQuery client object.\n",
        "client = bigquery.Client.from_service_account_json('/content/bq_service_account.json')\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    schema=[\n",
        "            bigquery.SchemaField('customer', 'INTEGER', 'REQUIRED', None, ()),\n",
        "            bigquery.SchemaField('amount_spent', 'NUMERIC', 'REQUIRED', None, ()),\n",
        "            bigquery.SchemaField('review_score', 'INTEGER', 'REQUIRED', None, ()),\n",
        "            bigquery.SchemaField('review_count', 'INTEGER', 'REQUIRED', None, ()),\n",
        "            bigquery.SchemaField('insert_date', 'TIMESTAMP', 'REQUIRED', None, ())],\n",
        "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
        ")\n",
        "\n",
        "# TODO(developer): Set table_id to the ID of the table to create.\n",
        "table_id = \"wizeline-bootcamp-330020.dwh.user_behavior_metric\"\n",
        "table = client.get_table(table_id)  # Make an API request.\n",
        "schema = table.schema "
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqHEq4lJ1WYY",
        "outputId": "28b90a26-103a-4fe6-96ce-adf0ba2b0b43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "job = client.load_table_from_dataframe(\n",
        "    pd_df, table_id, job_config=job_config\n",
        ")  # Make an API request.\n",
        "job.result()  # Wait for the job to complete.\n",
        "\n",
        "table = client.get_table(table_id)  # Make an API request.\n",
        "print(\n",
        "    \"Loaded {} rows and {} columns to {}\".format(\n",
        "        table.num_rows, len(table.schema), table_id\n",
        "    )\n",
        ")"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ArrowInvalid",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-d7664faf2e04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m job = client.load_table_from_dataframe(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpd_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )  # Make an API request.\n\u001b[1;32m      4\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for the job to complete.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mload_table_from_dataframe\u001b[0;34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression)\u001b[0m\n\u001b[1;32m   1608\u001b[0m                     \u001b[0mjob_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m                     \u001b[0mtmppath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m                     \u001b[0mparquet_compression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_compression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m                 )\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/_pandas_helpers.py\u001b[0m in \u001b[0;36mdataframe_to_parquet\u001b[0;34m(dataframe, bq_schema, filepath, parquet_compression)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow is required for BigQuery schema conversion.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0marrow_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe_to_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrow_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_compression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/_pandas_helpers.py\u001b[0m in \u001b[0;36mdataframe_to_arrow\u001b[0;34m(dataframe, bq_schema)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0marrow_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbq_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         arrow_arrays.append(\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mbq_to_arrow_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_column_or_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         )\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/bigquery/_pandas_helpers.py\u001b[0m in \u001b[0;36mbq_to_arrow_array\u001b[0;34m(series, bq_field)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbq_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_STRUCT_TYPES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrow_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrow_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mArrowInvalid\u001b[0m: Got bytestring of length 8 (expected 16)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFaQdTKg28yz"
      },
      "source": [
        "### Get table schema and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7HpnLoW1Zgq",
        "outputId": "8b3bcfb8-8b7c-4ce6-d221-456fa1625e0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "table = client.get_table(table_id)  # Make an API request.\n",
        "\n",
        "# View table properties\n",
        "print(\n",
        "    \"Got table '{}.{}.{}'.\".format(table.project, table.dataset_id, table.table_id)\n",
        ")\n",
        "print(\"Table schema: {}\".format(table.schema))\n",
        "print(\"Table description: {}\".format(table.description))\n",
        "print(\"Table has {} rows\".format(table.num_rows))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got table 'wizeline-bootcamp-330020.dwh.user_behavior_metric'.\n",
            "Table schema: [SchemaField('customer', 'INTEGER', 'REQUIRED', None, ()), SchemaField('amount_spent', 'NUMERIC', 'REQUIRED', None, ()), SchemaField('review_score', 'INTEGER', 'REQUIRED', None, ()), SchemaField('review_count', 'INTEGER', 'REQUIRED', None, ()), SchemaField('insert_date', 'TIMESTAMP', 'REQUIRED', None, ())]\n",
            "Table description: None\n",
            "Table has 0 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeMpRWRc2ztu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}